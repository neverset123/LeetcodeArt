1. KNN
用于特征与目标类之间的关系为比较复杂的数字类型，或者说二者关系难以理解，但是相似类间特征总是相似。k一般采用交叉验证确定。
- 根据给定的距离度量，在训练集 $T$ 中找出与 $x$ 最邻近的 $k$个点，涵盖这 $k$ 个点的邻域记作 $N_k(x)$；
- 根据多数表决决定x的类别
常用的距离度量：L_p(x_i, x_j) = (\sum_{l=1}^n |x_i^{(l)}-x_j^{(l)}|)^{\frac{1}{p}}
- 欧式距离（p=2）：适用于向量各分量的度量标准统一的情况
- 曼哈顿距离（p=1）：适用于计算类似街区距离这样的实际问题。异常值对分类结果影响比欧式距离小。量纲不同时使用曼哈顿距离比欧式距离好。
- 切比雪夫距离（$p = \infty$）：各个坐标距离数值差的绝对值的最大值
- 马氏距离：解决维度分布差异/量纲不同(different variances)的问题
- 汉明距离：两个等长字符串之间的汉明距离是两个字符串对应位置的不同字符的个数
- 相关系数：相关系数的绝对值越接近1，表示样本越相似；越接近0，表示样本越不相似。
- 余弦相似度：强调方向上的相对误差
- KL散度：计算两个分布的差异性
1）kd树
一种对k维空间中的实例点进行存储，以便对其进行快速检索的树形数据结构。
    - 选择$x^{(1)} $ 为坐标轴，以T中所有实例的 $x^{(1)} $ 坐标的中位数为切分点，将根节点对应的超矩形区域且分为两个子区域。切分由通过切分点并与坐标轴 $x^{(1)}$垂直的超平面实现
    - 重复：对深度为$j$ 的节点，选择 $x^{(l)}$ 为切分的坐标轴，$l = j({\rm mod}\ k) + 1$，以该节点的区域中所有实例的 $x^{(l)}$ 坐标的中位数为切分点，将该节点对应的超矩形区域切分为两个子区域。切分由通过切分点并与坐标轴 $x^{(l)}$垂直的超平面实现。
    - 直到两个子区域没有实例存在时停止，从而形成kd树的区域划分
2）kd树的最近邻搜索
kd树更适用与训练实例数远大于空间维数时的k近邻搜索。当空间维数接近训练实例数时，它的效率会迅速下降，几乎接近线性扫描。
    - 在kd树中从上到下找出包含目标点$x$的叶节点
    - 以此叶节点为“当前最近点”,递归地向上回退
        + 如果该节点保存的实例点比当前最近点距离目标点更近，则以该实例点作为“当前最近点”
        + 检查兄弟节点区域内是否存在更近点
    - 当回退到根节点时，搜索结束

2. SVM
在特征空间的间隔最大的线性分类器。对缺失值敏感（因为涉及到距离度量）。
样本中距离超平面最近的一些点，这些点叫做支持向量（n维特征空间会产生n+1个支持向量）。
核函数将样本映射到更高维的特征空间中，增大线性可分的可能性
    - 线性核函数
    - 高斯核函数
    - Sigmoid核函数
特征数量核样本数量大时，选用线性SVM；当特征数量小时，选用SVM+高斯核函数
3. 朴素贝叶斯模型
朴素贝叶斯假设所有的属性都是互相独立的。朴素贝叶斯将实例分到后验概率最大的类中。后验概率最大化这等价于期望风险最小化。是线性模型分类器（可以表示为特征值的加权线性组合，在高维样本空间中找到一组超平面，将样本空间划分了两个区域）。
计算过程：
    - 计算先验概率和条件概率
    - 计算联合概率
    - 通过最大化后验概率确定类别
贝叶斯网络
贝叶斯网络借助**有向无环图**来刻画属性之间的依赖关系，并使用**条件概率表**来描述属性的联合概率分布。
    - 利用贪心法，从某个网络结构出发，每次调整一条边，直到评分函数值不再降低
    - 给网络结构施加约束条件：如限定为树形结构等
4. 线性回归
根据数据集（feature+label）模拟出一条近似的曲线，使得每个点的落点都在曲线上或者是曲线的周围不远的地方。当自变量和因变量线性关系比较强时,适合选用。
损失函数是MSE，优化方法有
    - 正规方程
    - 梯度下降
5. 逻辑回归
在线性回归模型中引入Sigmoid函数，将线性回归的不确定范围的连续输出值映射到（0，1）范围内，成为一个概率预测问题。
处理多标签分类
    - 每个样本对应一个标签：使用softmax进行分类
    - 每个样本可能对应多个标签：训练多个二分类器
逻辑回归取对数好处
    - 防止概率乘积向下溢出
逻辑回归一般使用离散化的特征（连续特征离散化）
    - 离散化之后得到的稀疏向量，内积乘法运算速度更快，计算结果方便存储
    - 逻辑回归属于广义线性模型，表达能力受限，只能描述线性关系。特征离散化之后，相当于引入了非线性，提升模型的表达能力，增强拟合能力
    - 离散化之后可以进行特征交叉, 引入了 $M+N$ 个特征, 提高模型表达能力
    - 特征离散化简化了逻辑回归模型，同时降低模型过拟合的风险

6. 决策树
建树过程：
自上而下，对样本数据进行树形分类的过程。每个内部节点表示一个特征，叶节点表示一个类别。从顶部根节点开始，所有的样本聚在一起。经过根节点的划分，样本被分到不同的子节点，再根据子节点的特征进一步划分，直至所有样本被归到某一个类别（叶节点）中。

常见的决策树
| 不同点   | ID3                  | C4.5               | CART                   |
| -------- | -------------------- | ------------------ | ---------------------- |
| 原则     | 信息增益最大         | 信息增益比最大     | 划分后集合基尼指数最小 |
| 用途     | 分类                 | 分类               | 分类、回归             |
| 输入取值 | 离散                 | 离散、连续         | 离散、连续             |
| 树结构   | 多叉树               | 多叉树             | 二叉树                 |
|          | 特征在层级间不复用   | 特征在层级间不复用 | 每个特征可被重复利用   |
|          | 对样本特征缺失值敏感 |        

防止过拟合
通过**剪枝**防止过拟合
- 预剪枝
指在决策树生成的过程中，对每个节点在划分前先进行估计，若当前节点的划分不能带来决策树泛化性能提升，则停止划分，并将当前节点标记为叶子节点；此时可能存在不同类别的样本同时存于同个节点中，按照多数投票的原则判断节点所属类别。停止生长标准：
    + 当树达到一定深度
    + 当到达当前节点的样本数量小于某个阈值
    + 计算每次分裂对测试集的准确度提升，小于某个阈值时停止
- 后剪枝
先从训练集生成一棵完整的决策树，然后自底向上地对**非叶子节点**进行考察，若该节点对应的**子树替换成叶子结点**能带来泛化性能提升，则将该子树替换为叶子节点。

7. 随机森林（RF）
随机性体现在随机选取训练样本和随机选取分裂属性集合。
生成步骤
    - 用有抽样放回的方法（bootstrap）从样本集中选取n个样本作为一个训练集
    - 用抽样得到的样本集生成一棵决策树。在生成的每一个结点：
        + 随机不重复地选择d个特征
        + 利用这d个特征分别对样本集进行划分，找到最佳的划分特征（可用基尼系数、增益率或者信息增益判别）
    - 重复步骤1到步骤2共k次，k即为随机森林中决策树的个数。
    - 用训练得到的随机森林对测试样本进行预测，并用票选法决定预测的结果。

**处理缺失值**
    - 暴力填补
    如果是类别变量缺失，则用众数补全，如果是连续变量，则用中位数
    - 相似度矩阵填补
        + 首先先用暴力填补法进行粗粒度填充。
        + 然后使用上述填补后的训练集来训练随机森林模型，并统计相似度矩阵（proximity matrix），然后再看之前缺失值的地方，如果是分类变量，则用没有缺失的观测实例的相似度中的权重进行投票；如果是连续性变量，则用相似度矩阵进行加权求均值。
        + 上述投票方案迭代进行4~6次。
**特征重要性**
判断每个特征在随机森林中的每颗树上做了多大的贡献，然后取个平均值，最后比一比特征之间的贡献大小。贡献的计算方式
    - 基尼指数
    特征X在节点m上的重要性，即节点m分枝前后的Gini指数变化量; 对包含特征X的节点集合和树集合的Gini指数变化量求和做归一化处理。可以调用sklearn中importances = forest.feature_importances_

    - 袋外数据错误率
    对于一棵树 T_i，用OOB样本可以得到误差 e1，然后随机改变OOB中的第 j 列，保持其他列不变，对第 j 列进行随机的上下置换，得到误差 e2。至此，可以用 e1-e2 来刻画特征 j 的重要性。
    打乱方法
        + 使用uniform或者gaussian抽取随机值替换原特征
        + 通过permutation的方式将原来的所有N个样本的第 i 个特征值重新打乱分布

优点：
    - 所有的数据都能够有效利用，不用人为来做cross-validation；
    - 很少的参数可以实现很高的精确度；
    - 不用担心过拟合；
    - 随机选取特征来训练树，不用事先做特征选择。
缺点：
    - 速度慢。
