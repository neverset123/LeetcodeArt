## KNN

用于特征与目标类之间的关系为比较复杂的数字类型，或者说二者关系难以理解，但是相似类间特征总是相似。k一般采用交叉验证确定。

- 根据给定的距离度量，在训练集 $T$ 中找出与 $x$ 最邻近的 $k$个点，涵盖这 $k$ 个点的邻域记作 $N_k(x)$；
- 根据多数表决决定x的类别
  常用的距离度量：L_p(x_i, x_j) = (\sum_{l=1}^n |x_i^{(l)}-x_j^{(l)}|)^{\frac{1}{p}}
- 欧式距离（p=2）：适用于向量各分量的度量标准统一的情况
- 曼哈顿距离（p=1）：适用于计算类似街区距离这样的实际问题。异常值对分类结果影响比欧式距离小。量纲不同时使用曼哈顿距离比欧式距离好。
- 切比雪夫距离（$p = \infty$）：各个坐标距离数值差的绝对值的最大值
- 马氏距离：解决维度分布差异/量纲不同(different variances)的问题
- 汉明距离：两个等长字符串之间的汉明距离是两个字符串对应位置的不同字符的个数
- 相关系数：相关系数的绝对值越接近1，表示样本越相似；越接近0，表示样本越不相似。
- 余弦相似度：强调方向上的相对误差
- KL散度：计算两个分布的差异性
  1）kd树
  一种对k维空间中的实例点进行存储，以便对其进行快速检索的树形数据结构。
  - 选择$x^{(1)} $ 为坐标轴，以T中所有实例的 $x^{(1)} $ 坐标的中位数为切分点，将根节点对应的超矩形区域且分为两个子区域。切分由通过切分点并与坐标轴 $x^{(1)}$垂直的超平面实现
  - 重复：对深度为$j$ 的节点，选择 $x^{(l)}$ 为切分的坐标轴，$l = j({\rm mod}\ k) + 1$，以该节点的区域中所有实例的 $x^{(l)}$ 坐标的中位数为切分点，将该节点对应的超矩形区域切分为两个子区域。切分由通过切分点并与坐标轴 $x^{(l)}$垂直的超平面实现。
  - 直到两个子区域没有实例存在时停止，从而形成kd树的区域划分
    2）kd树的最近邻搜索
    kd树更适用与训练实例数远大于空间维数时的k近邻搜索。当空间维数接近训练实例数时，它的效率会迅速下降，几乎接近线性扫描。
  - 在kd树中从上到下找出包含目标点$x$的叶节点
  - 以此叶节点为“当前最近点”,递归地向上回退
    + 如果该节点保存的实例点比当前最近点距离目标点更近，则以该实例点作为“当前最近点”
    + 检查兄弟节点区域内是否存在更近点
  - 当回退到根节点时，搜索结束

## SVM

在特征空间的间隔最大的线性分类器。对缺失值敏感（因为涉及到距离度量）。
样本中距离超平面最近的一些点，这些点叫做支持向量（n维特征空间会产生n+1个支持向量）。
核函数将样本映射到更高维的特征空间中，增大线性可分的可能性

- 线性核函数
- 高斯核函数
- Sigmoid核函数
  特征数量核样本数量大时，选用线性SVM；当特征数量小时，选用SVM+高斯核函数

## 朴素贝叶斯模型

朴素贝叶斯假设所有的属性都是互相独立的。朴素贝叶斯将实例分到后验概率最大的类中。后验概率最大化这等价于期望风险最小化。是线性模型分类器（可以表示为特征值的加权线性组合，在高维样本空间中找到一组超平面，将样本空间划分了两个区域）。
计算过程：

- 计算先验概率和条件概率
- 计算联合概率
- 通过最大化后验概率确定类别
  贝叶斯网络
  贝叶斯网络借助**有向无环图**来刻画属性之间的依赖关系，并使用**条件概率表**来描述属性的联合概率分布。
- 利用贪心法，从某个网络结构出发，每次调整一条边，直到评分函数值不再降低
- 给网络结构施加约束条件：如限定为树形结构等

## 线性回归

根据数据集（feature+label）模拟出一条近似的曲线，使得每个点的落点都在曲线上或者是曲线的周围不远的地方。当自变量和因变量线性关系比较强时,适合选用。
损失函数是MSE，优化方法有

- 正规方程
- 梯度下降

## 逻辑回归

在线性回归模型中引入Sigmoid函数，将线性回归的不确定范围的连续输出值映射到（0，1）范围内，成为一个概率预测问题。
处理多标签分类

- 每个样本对应一个标签：使用softmax进行分类
- 每个样本可能对应多个标签：训练多个二分类器
  逻辑回归取对数好处
- 防止概率乘积向下溢出
  逻辑回归一般使用离散化的特征（连续特征离散化）
- 离散化之后得到的稀疏向量，内积乘法运算速度更快，计算结果方便存储
- 逻辑回归属于广义线性模型，表达能力受限，只能描述线性关系。特征离散化之后，相当于引入了非线性，提升模型的表达能力，增强拟合能力
- 离散化之后可以进行特征交叉, 引入了 $M+N$ 个特征, 提高模型表达能力
- 特征离散化简化了逻辑回归模型，同时降低模型过拟合的风险

## 决策树

建树过程：
自上而下，对样本数据进行树形分类的过程。每个内部节点表示一个特征，叶节点表示一个类别。从顶部根节点开始，所有的样本聚在一起。经过根节点的划分，样本被分到不同的子节点，再根据子节点的特征进一步划分，直至所有样本被归到某一个类别（叶节点）中。

常见的决策树

| 不同点   | ID3                  | C4.5               | CART                   |
| -------- | -------------------- | ------------------ | ---------------------- |
| 原则     | 信息增益最大         | 信息增益比最大     | 划分后集合基尼指数最小 |
| 用途     | 分类                 | 分类               | 分类、回归             |
| 输入取值 | 离散                 | 离散、连续         | 离散、连续             |
| 树结构   | 多叉树               | 多叉树             | 二叉树                 |
|          | 特征在层级间不复用   | 特征在层级间不复用 | 每个特征可被重复利用   |
|          | 对样本特征缺失值敏感 |                    |                        |

防止过拟合
通过**剪枝**防止过拟合

- 预剪枝
  指在决策树生成的过程中，对每个节点在划分前先进行估计，若当前节点的划分不能带来决策树泛化性能提升，则停止划分，并将当前节点标记为叶子节点；此时可能存在不同类别的样本同时存于同个节点中，按照多数投票的原则判断节点所属类别。停止生长标准：
  + 当树达到一定深度
  + 当到达当前节点的样本数量小于某个阈值
  + 计算每次分裂对测试集的准确度提升，小于某个阈值时停止
- 后剪枝
  先从训练集生成一棵完整的决策树，然后自底向上地对**非叶子节点**进行考察，若该节点对应的**子树替换成叶子结点**能带来泛化性能提升，则将该子树替换为叶子节点。

## 随机森林（RF）

随机性体现在随机选取训练样本和随机选取分裂属性集合。
生成步骤

- 用有抽样放回的方法（bootstrap）从样本集中选取n个样本作为一个训练集
- 用抽样得到的样本集生成一棵决策树（cart）。在生成的每一个结点：
  + 随机不重复地选择d个特征
  + 利用这d个特征分别对样本集进行划分，找到最佳的划分特征（可用基尼系数、增益率或者信息增益判别）
- 重复步骤1到步骤2共k次，k即为随机森林中决策树的个数。
- 用训练得到的随机森林对测试样本进行预测，并用票选法决定预测的结果。
  **损失函数**
- 分类RF对应的CART分类树默认是基尼系数gini,另一个可选择的标准是信息增益
- 回归RF对应的CART回归树默认是均方差mse，另一个可以选择的标准是绝对值差mae

**处理缺失值**
    - 暴力填补
    如果是类别变量缺失，则用众数补全，如果是连续变量，则用中位数
    - 相似度矩阵填补
        + 首先先用暴力填补法进行粗粒度填充。
        + 然后使用上述填补后的训练集来训练随机森林模型，并统计相似度矩阵（proximity matrix），然后再看之前缺失值的地方，如果是分类变量，则用没有缺失的观测实例的相似度中的权重进行投票；如果是连续性变量，则用相似度矩阵进行加权求均值。
        + 上述投票方案迭代进行4~6次。
**特征重要性**
判断每个特征在随机森林中的每颗树上做了多大的贡献，然后取个平均值，最后比一比特征之间的贡献大小。贡献的计算方式
    - 基尼指数
    特征X在节点m上的重要性，即节点m分枝前后的Gini指数变化量; 对包含特征X的节点集合和树集合的Gini指数变化量求和做归一化处理。可以调用sklearn中importances = forest.feature_importances_

    - 袋外数据错误率
    对于一棵树 T_i，用OOB样本可以得到误差 e1，然后随机改变OOB中的第 j 列，保持其他列不变，对第 j 列进行随机的上下置换，得到误差 e2。至此，可以用 e1-e2 来刻画特征 j 的重要性。
    打乱方法
        + 使用uniform或者gaussian抽取随机值替换原特征
        + 通过permutation的方式将原来的所有N个样本的第 i 个特征值重新打乱分布

优点：
    - 所有的数据都能够有效利用，不用人为来做交叉验证；
    - 很少的参数可以实现很高的精确度；
    - 不用担心过拟合；
    - 随机选取特征来训练树，不用事先做特征选择。
缺点：
    - 速度慢。
**弱分类器组合**
弱分类器更容易学习到局部特征，达到预测结果的多样性。集成学习潜是即便某一个弱分类器得到了错误的预测，其他的弱分类器也可以将错误纠正回来。
    - 数据集大：划分成多个小数据集，学习多个模型进行组合
    - 数据集小：利用Bootstrap方法进行抽样，得到多个数据集，分别训练多个模型再进行组合
集成方法
    - bagging（bootstrap aggregating，装袋）
    并行训练弱分类器，投票分类或均值回归。通过降低基分类器的方差，改善了泛化误差。如随机森林， 对n个独立不相关的模型的预测结果取平均，方差是原来单个模型的 $1/n$
    - boosting
    通过加法模型将弱分类器进行线性组合，根据错误率改变权值，降低偏差。如Adaboost和GBDT
    - stacking
    将弱分类器的输出作为输入通过一个模型进行组合来改进预测，通常使用logistic回归作为组合策略。
bagging和boosting区别
    - 样本选择
        + bagging训练样本相互独立
        + boosting每一轮训练集不变，样例的权值根据上一轮的分类结果发生变化
    - 样例权重
        + bagging均匀采样，权重相等
        + boosting错误率大权重大
    - 预测函数
        + bagging所有预测函数权重相等
        + boosting分类误差小的分类器权重大
    - 并行计算
        + bagging预测函数可以并行生成
        + boosting预测函数只能顺序生成

随机森林属于bagging类的集成学习方法，主要好处是减小集成后分类器的方差，比基分类器的方差小。所以Bagging所采用的的基分类器最好是本身对样本分布较为敏感（不稳定分类器）；线性分类器和KNN属于较为稳定的分类器，本身方差不大，不适用做bagging的基分类器。

## GBDT

每次都以当前预测为基准，下一个弱分类器去拟合误差函数对预测值的残差（预测值与真实值之间的误差），直到残差收敛到某个阈值以下，或者树的总数达到某个上限为止。所有弱分类器的结果相加等于预测值。
损失函数

- 分类模型: 对数损失
- 回归模型：MSE，MAE，Huber损失
  梯度提升vs梯度下降
- 梯度下降：模型是以参数化形式表示，从而模型的更新等价于参数的更新
- 梯度提升：模型并不需要进行参数化表示，而是直接定义在函数空间中，从而大大扩展了可以使用的模型种类。
  GBDT整体式串行运行的，但在决策树内部可以局部并行：
- 计算更新每个样本的负梯度
- 节点增益计算
- 预测过程可以并行
  优点
- 精度高
- 具有较好的可解释性和鲁棒性，能够自动发现特征间的高阶关系，并且不需要对数据进行特殊的预处理如归一化等
  缺点
- 计算复杂度大
- 对异常值敏感（当前的错误会延续给下一棵树）
  防止过拟合
- 控制tree的棵树，即迭代次数M
- 降低学习率
- 随机采样迭代
- 控制叶子节点的最少样本数
  重要参数及影响
- n_estimator：n_estimators太小，容易欠拟合，n_estimators太大，容易过拟合
- learning_rate：
- subsample：随机森林使用的是放回抽样，而这里是不放回抽样。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差

## k-means

kmeans聚类是基于样本集合划分的聚类算法。kmeans的算法复杂度是$O(imnk)$，接近线性，其中i为迭代次数，m是样本个数，n是数据维度，k是类别个数,比层次聚类复杂度低。对异常值敏感(因为需要距离计算)。
建模过程

- 首先随机选择k个样本点作为初始聚类中心
- 计算每个样本到类中心的距离，将样本逐个指派到与其最近的类中，得到一个聚类结果
- 更新每个类的样本的均值，作为新的中心
- 重复以上步骤，直到聚类中心不再改变，收敛为止

损失函数:样本与所属类的中心之间的距离的总和为损失函数。
初始类中心选择：可以用层次聚类对样本进行聚类，得到k个类时停止。然后从每个类中选取一个与中心距离最近的点
类别数选择
类别数变小时，平均直径会增加；类别数变大超过某个值后，平均直径会不变；而这个值是最优的k值。实验时可以采用二分查找，快速找到最优的k值。
    - 手肘法
    尝试不同的K值，并将不同K值所对应的损失函数化成折线，横轴为K的取值，纵轴为误差平方和所定义的损失函数。当 $K\in(1,K')$，曲线急速下降；当 $K>K'$，曲线趋于平稳。手肘法认为拐点就是K的最佳值。
    - Gap Statistic
    当分为K簇时，对应的损失函数记为 $D_k$，$E({\rm log}D_k)$是 ${\rm log}D_k$ 的期望，通过蒙特卡洛模拟产生。$\text{Gap}(K) $ 的物理含义是随机样本的损失与实际样本的损失之差。当$\text{Gap}(K) $取最大时，是样本的损失应该相对较小。
    {\rm Gap}(K) = E({\rm log}D_k) - {\rm log}D_k

提升效率
    - 使用kd树以及ball树对KNN计算优化
    - 并行计算

## PCA

变量之间可能存在相关性，以致增加了分析的难度。考虑用少数不想管的变量来替代相关的变量，用来表示数据，并且要求能保留数据中的大部分信息。
PCA利用正交变换把线性相关变量表示的观测数据转换为少数几个由线性无关变量表示的数据，线性无关的变量（协方差矩阵的特征向量）称为主成分。
PCA目标函数：最大化投影方差。因为方差表示新变量的信息量大小。
局限性与优化方法
1）无法进行非线性降维：通过核映射对PCA进行扩展得到核主成分分析（KPCA）
2）无监督的，算法没有考虑数据的标签：使用线性判别分析LDA（有监督的降维方法）
    - PCA选择的是投影后数据方差最大的方向。由于PCA是无监督的，因此假设方差越大，信息量越多，用主成分来表示原始数据可以去除冗余的维度，达到降维。
    - LDA选择的是投影后类内方差最小，类间方差最大的方向。其用到了类别标签信息，为了找到数据中具有判别性的维度，使得原始数据在这些方向投影后，不同类别尽可能区分开。
