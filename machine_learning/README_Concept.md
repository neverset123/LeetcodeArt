## 损失函数: 
单个样本的预测值与真实值的差称为损失。在回归问题中用残差[y-f(x)]表示，在分类问题中用趋势一致yf(x)表示。

- 回归损失
  - 平方损失：求导方便，能够用梯度下降法优化；对异常值敏感
  - 绝对损失：异常点多的情况下鲁棒性好；但不方便求导
  - Huber损失：结合了绝对损失函数和平方损失函数的优点；缺点是需要调整超参数
- 分类损失
  - 0-1损失函数：损失不连续，非凸，不宜优化
  - 对数损失函数/对数似然损失函数：
  - Log-Cosh 损失函数：具有Huber的所有优点，同时二阶处处可微（牛顿法要求二阶可微）

## 混淆矩阵
分别统计分类模型归错类，归对类的观测值个数，然后把结果放在一个表里展示出来。
## 拟合
   欠拟合解决方法：

   - 增加特征
   - 提高模型复杂度：神经网络提高神经元数、增加层数；SVM使用核函数；
   - 减小正则项的系数
     过拟合解决方法：
   - 提高样本数量 ：
   - 简化模型：
     - 神经网络使用 Dropout、Early Stopping
     - 决策树剪枝、限制树的深度
   - 加入正则化项（L1或L2）或提高惩罚系数
   - 使用集成学习
   - 神经网络中使用dropout机制
   - early stopping
   - 标签平滑
## 超参数
   超参搜索算法一般包括的要素（1）目标函数（2）搜索范围，上限和下限（3）其他参数，如搜索步长

   - 网格搜索：查找搜索范围内所有的点来确定最优值。简单但是耗时
   - 随机搜索：在搜索范围中随机选取样本点
   - 贝叶斯优化：对目标函数形状进行学习，找到使目标函数向全局最优值提升的参数。利用pre信息，但是容易陷入局部最优值
## 生成式模型和判别式模型的区别

   - 生成方法由数据学习联合概率分布  $P(X,Y)$，然后求出条件概率分布  $P(Y|X)$作为预测模型。如朴素贝叶斯法、隐马尔可夫模型
   - 判别方法由数据直接学习决策函数 $f(X)$ 或者条件概率分布 $P(X,Y)$作为预测的模型，关心的是对给定的输入 $X$，应该预测什么样的输出 $Y$。如k近邻、感知机、决策树、逻辑回归、支持向量机、提升方法
## 经验风险和结构风险
   - 经验风险是模型对于训练数据的平均损失；
   - 结构风险是经验风险家伤模型复杂度的正则项
## 参数化模型和非参数化模型
   - 参数化模型假设模型参数的维度固定，模型可以由有限维参数完全刻画(如感知机、朴素贝叶斯、逻辑回归、k均值、高斯混合模型)
   - 非参数模型假设模型参数的维度不固定或者说无穷大，随着训练数据量的增加而不断增大(决策树、支持向量机、AdaBoost、k近邻)
## 归一化和标准化
   概率模型不需要归一化/标准化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、RF。而像Adaboost、GBDT、XGBoost、SVM、LR、KNN、KMeans之类的最优化问题就需要归一化/标准化
## 类别型数据编码
   - 序号编码: 用于类别之间有大小关系的数据
   - one-hot编码：用于类别之间不具有大小关系的数据
   - 二进制编码：先序号编码，然后将序号二进制化。维度少于one-hot,节省存储空间
## 组合特征
    将一阶特征变为二阶特征，提高特征的非线性表达能力

    - 特征相乘
    - 特征相除
    - 特征分桶
      特征组合后如果维度很高，可以采取数据降维进行特征筛选
## 图像特征提取方法
    1）传统方法

    - HOG特征
      方向梯度直方图,通过计算和统计图像局部区域的梯度方向直方图来构成特征,结合SVM分类器用于图像识别
    - LBP特征
      局部二值模式是一种用来描述图像局部纹理特征的算子,具有旋转不变性和灰度不变性。一般用LBP特征谱的统计直方图作为特征向量用于分类识别
    - Haar特征
      分为三类：边缘特征、线性特征、中心特征和对角线特征，组合成特征模板。特征模板内有白色和黑色两种矩形，并定义该模板的特征值为白 色矩形像素和减去黑色矩形像素和。Haar特征值反映了图像的灰度变化情况。
      2）深度学习
## 特征选择
    - 过滤式filter
      运用统计指标来为每个特征打分并筛选特征，再用过滤后的特征来训练模型,特征选择过程与后续学习器无关。
      + 移除低方差特征
      + 相关系数排序
      + 假设检验计算相关性
        * 卡方检验
          先假设两个变量确实是独立的（“原假设”）,然后观察实际值（观察值）与理论值（这个理论值是指“如果两者确实独立”的情况下应该有的值）的偏差程度。卡方值越大，P值越小，相关性越强，对Y的解释性越好。特征为连续型，可将其分箱，变成有序的类别型特征，然后和label计算卡方值。
          sklearn-learn中有feature_selection.chi2方法支持大量特征进行卡方检验
        * F检验
          用来判断特征与label的相关性的，F 检验只能表示线性相关关系。
          scikit-learn 中提供了两种F检验方法 —— 适用于分类的 f_classif 和适用于回归的 f_regression ，分别对应单因素方差分析和线性相关分析
      + 互信息（信息增益）
        越大表示两个变量相关性越大。互信息能很好展现 x 和 y 之间的非线性关系。sklearn中normalized_mutual_info_score可以计算互信息。
        ![](https://raw.githubusercontent.com/neverset123/cloudimg/master/Img20230519211654.png)
        它不属于度量方式，也没有办法归一化，在不同数据及上的结果无法做比较；对于连续变量的计算通常变量需要先离散化
      + IV值
        用来对输入变量进行编码和预测能力评估，取值范围是【0, 正无穷）。
        单个分组IV值：
        ![](https://raw.githubusercontent.com/neverset123/cloudimg/master/Img20230519202318.png)
        整个变量的IV值：
        ![](https://raw.githubusercontent.com/neverset123/cloudimg/master/Img20230519202511.png)
    - 包裹式wrapper
      从初始特征集合中不断的选择特征子集（如LVM），训练学习器，根据学习器的性能来对子集进行评价，直到选择出最佳的子集。效果好，但是训练开销大
    - 嵌入式embedding
      在学习器训练过程中自动地进行特征选择。嵌入式选择最常用的是L1正则化和L2正则化，正则化项越大，模型越简单，系数越小。逻辑回归、线性回归、决策树都可以当作正则化选择特征的基学习器，只有可以得到特征系数或者可以得到特征重要度的算法才可以作为嵌入式选择的基学习器。
## 特征相关性

    - 协方差
      只表示线性相关的方向，取值正无穷到负无穷。dataframe.cov()计算所有变量之间的协方差；series.cov(series)计算指定变量之间的协方差
    - pearson相关系数
      不仅表示线性相关的方向，还表示线性相关的程度，取值[-1,1]。pearson相关系数为0并不一定表示两个变量之间是独立的，也有可能是非线性相关的。dataframe.corr()计算所有变量之间的相关系数；series.corr(series)计算指定变量之间的协方差
    - 距离相关系数
      研究两个变量之间的独立性，距离相关系数为0表示两个变量是独立的。克服了皮尔逊相关系数（Pearson）的弱点。
    - MIC最大信息系数
      检测变量之间非线性相关性，值域在 0 和 1 之间
      对于小数据集的MIC计算没有意义，因为误差会非常大。
## 类别不平衡问题
    对于类别不平衡问题，常用的有三种方法：

- 基于再缩放策略进行决策，称之为阈值移动 `threshold-moving` 。
- 直接对训练集里的大类样本进行欠采样 `undersampling`。
- 直接对训练集里的小类样本进行过采样 `oversampling`。

1）再缩放
在进行预测的时候，令：

$$
\frac{\overline{p}}{1-\overline{p}}=\frac{p}{1-p} \times \frac{N^{-}}{N^{+}}
$$

然后再将 $\overline{p}$ 跟阈值比较。由于“训练集是真实样本总体的无偏采样”这个假设往往不成立，所以无法基于训练集观测几率来推断出真实几率。
2）欠采样
常用方法是将大类划分成若干个集合供不同学习器使用，这样对每个学习器来看都是欠采样，但是全局来看并不会丢失重要信息。
3）过采样
通常在原始小类之间插值来生成额外的数据。如 `SMOTE`方法：对于每个正类样本 $\overrightarrow{\mathbf{x}}_{i}^{+}$ ，从它的 $k$ 近邻中随机选取一个样本点 $\hat{\mathbf{x}}_{i}^{+}$，然后根据下式生成一个新的正类样本：$\overrightarrow{\mathbf{x}}_{n e w}^{+}=\overrightarrow{\mathbf{x}}_{i}^{+}+\left(\hat{\mathbf{x}}_{i}^{+}-\overrightarrow{\mathbf{x}}_{i}^{+}\right) \times \delta$ ，其中 $\delta \in[0,1]$ 是随机数。

## 正则化
    L1和L2正则，都可以防止过拟合，增强模型的泛化能力；区别在于L1使参数更稀疏，达到特征选取的作用；L2使参数更接近于0。
    L1产生稀疏解的原因：L1正则项约束后的解空间是多边形，而L2正则项约束后的解空间是圆形。而多边形的解空间更容易在尖角处与等高线碰撞出稀疏解。
    对于存在线性相关的一组特征，L1正则会使得部分参数为0。
    L2正则化可以造成权重衰减，避免模型过拟合。（更小的权值代表更低的网络复杂度；拟合函数的导数值小,波动小）
## 熵
    熵（entropy）是表示随机变量不确定性的度量，是整个系统的平均信息量。
    H(X) = -\sum_{i=1}^{n} p_i {\rm log } \ p_i
    **自信息量**是用来描述某一条信息的大小

$$
I = - {\rm log} \ p_i
$$

通常以2为底，单位是bit；含义是用多少位二进制可以表示衡量该信息的大小。
    - 联合熵
    多维随机变量的熵
    - 条件熵
    $H(Y|X)$ 表示在已知随机变量X的条件下随机变量Y的不确定性。是指在给定某个数（某个变量为某个值）的情况下，另一个变量的熵是多少
    - 信息增益
    数据集D的经验熵 $H(D)$ 与特征A给定条件下D的经验条件熵 $H(D|A)$ 的差
    - 信息增益比
    因为信息增益对取值数目多的属性有所偏好，为了减少这种偏好带来的影响，使用信息增益比来选择最优划分属性。
    在信息增益的基础之上乘上一个惩罚参数。特征取值较多时，惩罚参数较小；特征取值较少时，惩罚参数较大。
    惩罚参数：数据集D以特征A作为随机变量的熵的倒数，即：将特征A取值相同的样本划分到同一个子集中（之前所说数据集的熵是依据类别进行划分的）。
    - 基尼系数
    表示在样本集合中一个随机选中的样本被分错的概率。基尼指数（基尼不纯度）= 样本被选中的概率 * 样本被分错的概率。CART在每一次迭代中选择划分后**基尼指数最小**的特征及其对应的切分点进行分类

    - 交叉熵
    刻画两个概率分布之间的距离，通过q来表示p的交叉熵为；一般p(x)为真实分布，q(x)为预测分布。
    交叉熵不对称。交叉熵越小，概率分布越接近

$$
H(p,q) = - \sum\limits_{x} p(x) {\rm log } \ q(x)
$$
    - KL散度/相对熵
    使用KL散度来评估predict和label之间的差别,与交叉熵等效

## 聚类评价指标
    聚类一般要求高的类内相似度和低的类间相似度。

- 聚类纯度（purity）
  把每个簇中最多的类作为这个簇所代表的类，然后计算正确分配的类的数量，然后除以 $N$。
  优点：方便计算，值在0~1之间；
  缺点：不能简单用纯度来衡量聚类质量与聚类数量之间的关系
- 兰德系数与F值
  计算混淆矩阵
  - TP：表示两个同类样本点在同一个簇（布袋）中的情况数量；
  - FP：表示两个非同类样本点在同一个簇中的情况数量；
  - TN：表示两个非同类样本点分别在两个簇中的情况数量；
  - FN：表示两个同类样本点分别在两个簇中的情况数量；
    ![](https://raw.githubusercontent.com/neverset123/cloudimg/master/Img20230521143748.png)
    RI和F的取值范围均为【0，1】，越大表示聚类效果越好
    为了去掉随机标签对于兰德系数评估结果的影响，可以使用调整兰德系数。
