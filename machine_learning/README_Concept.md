## 常见聚类算法
基于划分（kmeans），基于密度（DBScan），基于网格（Clique），层次聚类，除此之外聚类和其它领域也有很多的结合形成的交叉领域比如半监督聚类，深度聚类，集成聚类等等

## 优化算法
梯度下降法是一阶优化算法，牛顿法是二阶优化算法。牛顿法的收敛速度相比梯度下降法常常较快，但是计算开销大，实际中常用拟牛顿法。牛顿法对初始值有一定要求，在非凸优化问题中（如神经网络训练），牛顿法很容易陷入鞍点（牛顿法步长会越来越小），而梯度下降法则很容易逃离鞍点（因此在神经网络训练中一般使用梯度下降法，高维空间的神经网络中存在大量鞍点）
- 梯度下降法
  - 批量梯度下降法BGD
  使用所有的训练数据一起进行梯度的更新（梯度下降算法一般用来最小化损失函数）。缺点是计算量比较大。
  - 随机梯度下降法SGD
  每次拿出训练集中的一个数据，进行拟合训练，进行迭代去训练，直到参数比较稳定。优点是迭代速度快，缺点是抖动或收敛到局部最优。
  - 小批量梯度下降法MBGD
  每次输入网络训练数据集的一部分
- 牛顿法
利用迭代点 处的一阶导数(梯度)和二阶导数(Hessen矩阵)对目标函数进行二次函数近似，然后把二次模型的极小点作为新的迭代点，并不断重复这一过程，直至求得满足精度的近似极小值。

## 损失函数: 
单个样本的预测值与真实值的差称为损失。在回归问题中用残差[y-f(x)]表示，在分类问题中用趋势一致yf(x)表示。
梯度回传通常会占用两倍于前向的memory和compute。

- 回归损失

  - 平方损失：求导方便，能够用梯度下降法优化；对异常值敏感
  - 绝对损失：异常点多的情况下鲁棒性好；但不方便求导
  - Huber损失：结合了绝对损失函数和平方损失函数的优点；缺点是需要调整超参数
- 分类损失
  - 0-1损失函数：损失不连续，非凸，不宜优化
  - 对数损失函数/对数似然损失函数：
  - Log-Cosh 损失函数：具有Huber的所有优点，同时二阶处处可微（牛顿法要求二阶可微）
  $$L(y,y^p)=\sum_{i=1}^nlog(cosh(y_i^p-yi))$$
  - cross entropy: 概率乘积转化为相加.
    - 二分类
      - 单层网络
        $$
        CE = {-}\frac{1}{m}\sum_{i=1}^{m}(y_i\times \ln{\hat{y_i}}+(1-y_i)\times \ln(1-\hat{y_i}))
        $$
        $$
        \nabla{CE} = -(y-\hat{y})\times(x_1,...,x_n,1)
        $$
        对于含激活函数的分类问题
        $$
        \triangle{w_i} = \alpha*\delta*x_i = \alpha*(y-\hat{y})*f^{'}(\sum(w_i*x_i)) x_i
        $$
      - 多层网络  
        通过chain rule可以计算各层权重的偏微分
    - 多分类
      $$
      CE = {-}\frac{1}{m}\sum_{i=1}^{m}\sum_{j=1}^{n}(y_{ij}\times \ln{\hat{y_{ij}}})
      $$


## 混淆矩阵
- 混淆矩阵的每一列代表了预测类别，每一列的总数表示预测为该类别的数据的数目；
- 每一行代表了数据的真实归属类别，每一行的数据总数表示该类别的数据实例的数目；每一列中的数值表示真实数据被预测为该类的数目
## 拟合
欠拟合解决方法：

- 增加特征
- 提高模型复杂度：神经网络提高神经元数、增加层数；SVM使用核函数；
- 减小正则项的系数

过拟合解决方法：
- 提高样本数量 ：
- 简化模型：
  - 神经网络使用 Dropout、Early Stopping
  - 决策树剪枝、限制树的深度
- 加入正则化项（L1或L2）或提高惩罚系数
- 使用集成学习
- 神经网络中使用dropout机制（dropout会自动将保留层的概率*1/keep_prob，以保证训练和测试期望一致）
- early stopping
- 标签平滑
## 超参数
超参搜索算法一般包括的要素（1）目标函数（2）搜索范围，上限和下限（3）其他参数，如搜索步长

- 网格搜索：查找搜索范围内所有的点来确定最优值。简单但是耗时
- 随机搜索：在搜索范围中随机选取样本点
- 贝叶斯优化：对目标函数形状进行学习，找到使目标函数向全局最优值提升的参数。利用pre信息，但是容易陷入局部最优值
## 生成式模型和判别式模型的区别

- 生成方法由数据学习联合概率分布  $P(X,Y)$，然后求出条件概率分布  $P(Y|X)$作为预测模型。如朴素贝叶斯法、隐马尔可夫模型
- 判别方法由数据直接学习决策函数 $f(X)$ 或者条件概率分布 $P(X,Y)$作为预测的模型，关心的是对给定的输入 $X$，应该预测什么样的输出 $Y$。如k近邻、感知机、决策树、逻辑回归、支持向量机、提升方法
## 经验风险和结构风险
- 经验风险是模型对于训练数据的平均损失；
- 结构风险是经验风险家伤模型复杂度的正则项
## 参数化模型和非参数化模型
- 参数化模型假设模型参数的维度固定，模型可以由有限维参数完全刻画(如感知机、朴素贝叶斯、逻辑回归、k均值、高斯混合模型)
- 非参数模型假设模型参数的维度不固定或者说无穷大，随着训练数据量的增加而不断增大(决策树、支持向量机、AdaBoost、k近邻)
## 归一化和标准化
概率模型不需要归一化/标准化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、RF。而像Adaboost、GBDT、XGBoost、SVM、LR、KNN、KMeans之类的最优化问题就需要归一化/标准化
$$
归一化: X_{scale} = (X-max(X))/max(X)
标准化: X_{norm} = (X-mean(X)/std(X))
$$
* 对无监督学习算法使用标准化比归一化更有利。
* 数据中包含曲线，那么标准化是更可取的。
* 数据集具有极高或极低的值（离群值），则标准化是更可取的。

## 类别型数据编码
- 序号编码: 用于类别之间有大小关系的数据
- one-hot编码：用于类别之间不具有大小关系的数据
- 二进制编码：先序号编码，然后将序号二进制化。维度少于one-hot,节省存储空间
## 组合特征
将一阶特征变为二阶特征，提高特征的非线性表达能力

- 特征相乘
- 特征相除
- 特征分桶
  特征组合后如果维度很高，可以采取数据降维进行特征筛选
## 图像特征提取方法
1）传统方法

- HOG特征
  方向梯度直方图,通过计算和统计图像局部区域的梯度方向直方图来构成特征,结合SVM分类器用于图像识别
- LBP特征
  局部二值模式是一种用来描述图像局部纹理特征的算子,具有旋转不变性和灰度不变性。一般用LBP特征谱的统计直方图作为特征向量用于分类识别
- Haar特征
  分为三类：边缘特征、线性特征、中心特征和对角线特征，组合成特征模板。特征模板内有白色和黑色两种矩形，并定义该模板的特征值为白 色矩形像素和减去黑色矩形像素和。Haar特征值反映了图像的灰度变化情况。
  2）深度学习
## 特征选择
- 过滤式filter
  运用统计指标来为每个特征打分并筛选特征，再用过滤后的特征来训练模型,特征选择过程与后续学习器无关。
  + 移除低方差特征
  + 相关系数排序
  + 假设检验计算相关性
    * 卡方检验
      先假设两个变量确实是独立的（“原假设”）,然后观察实际值（观察值）与理论值（这个理论值是指“如果两者确实独立”的情况下应该有的值）的偏差程度。卡方值越大，P值越小，相关性越强，对Y的解释性越好。特征为连续型，可将其分箱，变成有序的类别型特征，然后和label计算卡方值。
      sklearn-learn中有feature_selection.chi2方法支持大量特征进行卡方检验
    * F检验
      用来判断特征与label的相关性的，F 检验只能表示线性相关关系。
      scikit-learn 中提供了两种F检验方法 —— 适用于分类的 f_classif 和适用于回归的 f_regression ，分别对应单因素方差分析和线性相关分析
  + 互信息（信息增益）
    越大表示两个变量相关性越大。互信息能很好展现 x 和 y 之间的非线性关系。sklearn中normalized_mutual_info_score可以计算互信息。
    ![](https://raw.githubusercontent.com/neverset123/cloudimg/master/Img20230519211654.png)
    它不属于度量方式，也没有办法归一化，在不同数据及上的结果无法做比较；对于连续变量的计算通常变量需要先离散化
  + IV值
    用来对输入变量进行编码和预测能力评估，取值范围是【0, 正无穷）。
    单个分组IV值：
    ![](https://raw.githubusercontent.com/neverset123/cloudimg/master/Img20230519202318.png)
    整个变量的IV值：
    ![](https://raw.githubusercontent.com/neverset123/cloudimg/master/Img20230519202511.png)
- 包裹式wrapper
  从初始特征集合中不断的选择特征子集（如LVM），训练学习器，根据学习器的性能来对子集进行评价，直到选择出最佳的子集。效果好，但是训练开销大
- 嵌入式embedding
  在学习器训练过程中自动地进行特征选择。嵌入式选择最常用的是L1正则化和L2正则化，正则化项越大，模型越简单，系数越小。逻辑回归、线性回归、决策树都可以当作正则化选择特征的基学习器，只有可以得到特征系数或者可以得到特征重要度的算法才可以作为嵌入式选择的基学习器。
## 特征相关性

- 协方差
  只表示线性相关的方向，取值正无穷到负无穷。dataframe.cov()计算所有变量之间的协方差；series.cov(series)计算指定变量之间的协方差
- pearson相关系数
  不仅表示线性相关的方向，还表示线性相关的程度，取值[-1,1]。pearson相关系数为0并不一定表示两个变量之间是独立的，也有可能是非线性相关的。dataframe.corr()计算所有变量之间的相关系数；series.corr(series)计算指定变量之间的协方差
- 距离相关系数
  研究两个变量之间的独立性，距离相关系数为0表示两个变量是独立的。克服了皮尔逊相关系数（Pearson）的弱点。
- MIC最大信息系数
  检测变量之间非线性相关性，值域在 0 和 1 之间
  对于小数据集的MIC计算没有意义，因为误差会非常大。
## 类别不平衡问题
对于类别不平衡问题，常用的有三种方法：

- 基于再缩放策略进行决策，称之为阈值移动 `threshold-moving` 。
- 直接对训练集里的大类样本进行欠采样 `undersampling`。
- 直接对训练集里的小类样本进行过采样 `oversampling`。

1）再缩放
在进行预测的时候，令：

$$
\frac{\overline{p}}{1-\overline{p}}=\frac{p}{1-p} \times \frac{N^{-}}{N^{+}}
$$

然后再将 $\overline{p}$ 跟阈值比较。由于“训练集是真实样本总体的无偏采样”这个假设往往不成立，所以无法基于训练集观测几率来推断出真实几率。
2）欠采样
常用方法是将大类划分成若干个集合供不同学习器使用，这样对每个学习器来看都是欠采样，但是全局来看并不会丢失重要信息。
3）过采样
通常在原始小类之间插值来生成额外的数据。如 `SMOTE`方法：对于每个正类样本 $\overrightarrow{\mathbf{x}}_{i}^{+}$ ，从它的 $k$ 近邻中随机选取一个样本点 $\hat{\mathbf{x}}_{i}^{+}$，然后根据下式生成一个新的正类样本：$\overrightarrow{\mathbf{x}}_{n e w}^{+}=\overrightarrow{\mathbf{x}}_{i}^{+}+\left(\hat{\mathbf{x}}_{i}^{+}-\overrightarrow{\mathbf{x}}_{i}^{+}\right) \times \delta$ ，其中 $\delta \in[0,1]$ 是随机数。

## 前景背景不均衡 
解决方法
* OHEM: 首先计算出每个ROI的loss， 然后按loss从高到低来排列每个 ROI。为了ROI重合冗余，先使用 NMS (non-maximum suppression) 删除部分重合度很高的 ROI。然后为每张图片选择 B/N 个损失最高的 ROI 作为Hard Examples，其中 B 表示总的 ROI 数量， N 表示batch-size 的大小。

  优点：
    - 对于数据的类别不平衡问题不需要采用设置正负样本比例的方式来解决，这种在线选择方式针对性更强；
  	随着数据集的增大，算法的提升更加明显；

  缺点：
    - 只保留loss较高的样本，完全忽略简单的样本，这本质上是改变了训练时的输入分布（仅包含困难样本）。
* Focalloss
Focal loss 通过改变 Cross loss来达到区分easy/hard的目的:
当 pt 非常小时，即样本被分类错误，此时 (1−pt)γ 接近1， loss几乎不受影响，当 pt 接近于1时，即样本被分类正确，此时 (1−pt)γ 接近0，此时降低了该样本的权重

  优点:
    - FocalLoss则是对正负样本进行加权，使得全部的样本可以得到学习，容易分类的负样本赋予低权值，hard examples赋予高权值

  缺点:
    - 在所有的anchor examples中，出了大量的易分类的负样本外，还存在很多的outlier，FocalLoss对这些outlier并没有相关策略处理。
    - FocalLoss存在两个超参，根据不同的数据集，调试两个超参需要大量的实验，一旦确定参数无法改变，不能根据数据的分布动态的调整
* 前景过采样: 增加样本数少的样本的采样方法

## 正则化
正则化本质代表特征表达项的数量。
多重共线性会造成最小二乘法求解的权值不稳定，正则化可以对权值进行约束。
L1和L2正则，都可以防止过拟合，增强模型的泛化能力；区别在于L1使参数更稀疏，达到特征选取的作用；L2使参数更接近于0。
L1产生稀疏解的原因：L1正则项约束后的解空间是多边形，而L2正则项约束后的解空间是圆形。而多边形的解空间更容易在尖角处与等高线碰撞出稀疏解。
对于存在线性相关的一组特征，L1正则会使得部分参数为0。
L2正则化可以造成权重衰减，避免模型过拟合。（更小的权值代表更低的网络复杂度；拟合函数的导数值小,波动小）
## 熵
熵（entropy）是表示随机变量不确定性的度量，是整个系统的平均信息量。
$$ 
H(X) = -\sum_{i=1}^{n} p_i {\rm log } \ p_i 
$$
**自信息量**是用来描述某一条信息的大小

$$
I = - {\rm log} \ p_i
$$

通常以2为底，单位是bit；含义是用多少位二进制可以表示衡量该信息的大小。
  - 联合熵
  多维随机变量的熵
  - 条件熵
  $H(Y|X)$ 表示在已知随机变量X的条件下随机变量Y的不确定性。是指在给定某个数（某个变量为某个值）的情况下，另一个变量的熵是多少
  - 信息增益
  数据集D的经验熵 $H(D)$ 与特征A给定条件下D的经验条件熵 $H(D|A)$ 的差
  - 信息增益比
  因为信息增益对取值数目多的属性有所偏好，为了减少这种偏好带来的影响，使用信息增益比来选择最优划分属性。
  在信息增益的基础之上乘上一个惩罚参数。特征取值较多时，惩罚参数较小；特征取值较少时，惩罚参数较大。
  惩罚参数：数据集D以特征A作为随机变量的熵的倒数，即：将特征A取值相同的样本划分到同一个子集中（之前所说数据集的熵是依据类别进行划分的）。
  - 基尼系数
  表示在样本集合中一个随机选中的样本被分错的概率。基尼指数（基尼不纯度）= 样本被选中的概率 * 样本被分错的概率。CART在每一次迭代中选择划分后**基尼指数最小**的特征及其对应的切分点进行分类

  - 交叉熵
  刻画两个概率分布之间的距离，通过q来表示p的交叉熵为；一般p(x)为真实分布，q(x)为预测分布。
  交叉熵不对称。交叉熵越小，概率分布越接近

$$
H(p,q) = - \sum\limits_{x} p(x) {\rm log } \ q(x)
$$
- KL散度/相对熵
使用KL散度来评估predict和label之间的差别,与交叉熵等效

## 聚类评价指标
聚类一般要求高的类内相似度和低的类间相似度。

- 聚类纯度（purity）
  把每个簇中最多的类作为这个簇所代表的类，然后计算正确分配的类的数量，然后除以 $N$。
  优点：方便计算，值在0~1之间；
  缺点：不能简单用纯度来衡量聚类质量与聚类数量之间的关系
- 兰德系数与F值
  计算混淆矩阵
  - TP：表示两个同类样本点在同一个簇（布袋）中的情况数量；
  - FP：表示两个非同类样本点在同一个簇中的情况数量；
  - TN：表示两个非同类样本点分别在两个簇中的情况数量；
  - FN：表示两个同类样本点分别在两个簇中的情况数量；
    ![](https://raw.githubusercontent.com/neverset123/cloudimg/master/Img20230521143748.png)
    RI和F的取值范围均为【0，1】，越大表示聚类效果越好
    为了去掉随机标签对于兰德系数评估结果的影响，可以使用调整兰德系数。

## 有监督学习评估指标
1）Fbeta
$$F_\beta = (1+\beta^2)*\frac{precision*recall}{(\beta^2*precision)+recall}$$

beta用于定义召回率和精确率的相对重要性，越大，则recall越重要，当beta趋于无穷大时，Fbeta=recall，越小，则precision越重要，当beta**2趋于0时，Fbeta=precision
2）ks曲线
ks曲线的横坐标是分类的阈值，纵坐标代表了精确率或者误杀率，一个分类阈值对应的一个精确率和一个误杀率，而ks曲线就是用每个分类阈值下的精确率-误杀率，ks值则是指ks曲线上的最大值

## PR曲线
根据置信度从大到小排序所有的预测框，计算累加的 Precision 和 Recall 的值, 绘制PR曲线，precision取右侧最大值做平滑处理。
![image info](../docs/img/PR_table.PNG)
计算 AP，一般使用的是插值的方法，取 11 个点 [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1] 的插值所得
$$AP=\frac{1}{11}*\sum{precision} $$

## ROC曲线
* 通过动态调整截断点，从最高置信度开始(对应ROC曲线零点)逐渐调整到最低得分，每个截断点对应一个FPR和TPR，在ROC图上绘制每个截断点的位置，连接所有点得到最终的ROC曲线。
* 根据样本标签统计正负样本数量(正样本数量为P，负样本数量为N)。把横轴刻度设置为1/N, 纵轴刻度设置为1/P，根据模型输出的预测概率对样本从高到低进行排序。依次遍历样本，从零点开始绘制POC曲线，每遇到一个正样本就沿纵轴绘制一个刻度间隔的曲线，每遇到一个负样本就沿横轴绘制一个刻度间隔的曲线，直到遍历完所有样本，曲线到达(1，1)。

## AUC
AUC(Area under Curve)：Roc曲线下的面积，介于0.5和1之间。Auc作为数值可以直观的评价分类器的好坏，值越大越好。(AUC是随机抽出一对样本，把正样本预测为1的概率大于把负样本预测为1的概率的概率)
AUC的计算方法同时考虑了分类器对于正例和负例的分类能力，在样本不平衡的情况下，依然能够对分类器作出合理的评价。
计算方法:
* 对所有样本按照预测值从小到大标记rank(index+1)。将所有正样本的rank相加，遇到预测值相等的情况，不管样本的正负性，对rank要取平均值再相加。将rank相加的和减去一个常数项，再除以总的组合数，得到auc。
$$AUC=\frac{\sum_{positiveclass}{rank}-\frac{M*(M+1)}{2}}{M*N}$$
rank表示第i个样本的序号，M，N分别表示正负样本个数
* 在有M个正样本N个负样本的数据集里。一共有M*N对样本（A，B），若P(A)>P(B)记为1，若P(A)<P(B)记为0，若P(A)=P(B)记为0.5。求和后除以总的组合数得到AUC。
$$Auc=\frac{\sum{score(P_{pos},P_{neg})}}{M*N}$$ 

| case  | score |
| -------- | ------------| 
| $$P_{pos}>P_{neg}$$ | 1 |
| $$P_{pos}<P_{neg}$$ | 0 | 
| $$P_{pos}=P_{neg}$$ |0.5| 

GAUC
用户推荐是一个个性化的场景，不同用户之间的商品排序不好放在一起比较，需要使用GAUC进行评价，
$$GAUC=\frac{\sum_{ui}{w_{ui}*AUC}}{\sum{w_{ui}}}$$