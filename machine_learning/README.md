1. 损失函数: 单个样本的预测值与真实值的差称为损失。在回归问题中用残差[y-f(x)]表示，在分类问题中用趋势一致yf(x)表示。
   - 回归损失
     - 平方损失：求导方便，能够用梯度下降法优化；对异常值敏感
     - 绝对损失：异常点多的情况下鲁棒性好；但不方便求导
     - Huber损失：结合了绝对损失函数和平方损失函数的优点；缺点是需要调整超参数
   - 分类损失
     - 0-1损失函数：损失不连续，非凸，不宜优化
     - 对数损失函数/对数似然损失函数：
     - Log-Cosh 损失函数：具有Huber的所有优点，同时二阶处处可微（牛顿法要求二阶可微）

2. 混淆矩阵，就是分别统计分类模型归错类，归对类的观测值个数，然后把结果放在一个表里展示出来。
3. 拟合
欠拟合解决方法：
    - 增加特征
    - 提高模型复杂度：神经网络提高神经元数、增加层数；SVM使用核函数；
    - 减小正则项的系数
过拟合解决方法：
    - 提高样本数量 ：
    - 简化模型：
        - 神经网络使用 Dropout、Early Stopping
        - 决策树剪枝、限制树的深度
    - 加入正则化项（L1或L2）或提高惩罚系数
    - 使用集成学习
    - 神经网络中使用dropout机制
    - early stopping
    - 标签平滑

4. 超参数
超参搜索算法一般包括的要素（1）目标函数（2）搜索范围，上限和下限（3）其他参数，如搜索步长
    - 网格搜索：查找搜索范围内所有的点来确定最优值。简单但是耗时
    - 随机搜索：在搜索范围中随机选取样本点
    - 贝叶斯优化：对目标函数形状进行学习，找到使目标函数向全局最优值提升的参数。利用pre信息，但是容易陷入局部最优值
5. 生成式模型和判别式模型的区别
    - 生成方法由数据学习联合概率分布  $P(X,Y)$，然后求出条件概率分布  $P(Y|X)$作为预测模型。如朴素贝叶斯法、隐马尔可夫模型
    - 判别方法由数据直接学习决策函数 $f(X)$ 或者条件概率分布 $P(X,Y)$作为预测的模型，关心的是对给定的输入 $X$，应该预测什么样的输出 $Y$。如k近邻、感知机、决策树、逻辑回归、支持向量机、提升方法
6. 经验风险和结构风险
    - 经验风险是模型对于训练数据的平均损失；
    - 结构风险是经验风险家伤模型复杂度的正则项
7. 参数化模型和非参数化模型
    - 参数化模型假设模型参数的维度固定，模型可以由有限维参数完全刻画(如感知机、朴素贝叶斯、逻辑回归、k均值、高斯混合模型)
    - 非参数模型假设模型参数的维度不固定或者说无穷大，随着训练数据量的增加而不断增大(决策树、支持向量机、AdaBoost、k近邻)
8. 归一化和标准化
概率模型不需要归一化/标准化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、RF。而像Adaboost、GBDT、XGBoost、SVM、LR、KNN、KMeans之类的最优化问题就需要归一化/标准化
9.  类别型数据编码
    - 序号编码: 用于类别之间有大小关系的数据
    - one-hot编码：用于类别之间不具有大小关系的数据
    - 二进制编码：先序号编码，然后将序号二进制化。维度少于one-hot,节省存储空间
10. 组合特征
将一阶特征变为二阶特征，提高特征的非线性表达能力
    - 特征相乘
    - 特征相除
    - 特征分桶
特征组合后如果维度很高，可以采取数据降维进行特征筛选
11. 图像特征提取方法
1）传统方法
    - HOG特征
    方向梯度直方图,通过计算和统计图像局部区域的梯度方向直方图来构成特征,结合SVM分类器用于图像识别
    - LBP特征
    局部二值模式是一种用来描述图像局部纹理特征的算子,具有旋转不变性和灰度不变性。一般用LBP特征谱的统计直方图作为特征向量用于分类识别
    - Haar特征
    分为三类：边缘特征、线性特征、中心特征和对角线特征，组合成特征模板。特征模板内有白色和黑色两种矩形，并定义该模板的特征值为白 色矩形像素和减去黑色矩形像素和。Haar特征值反映了图像的灰度变化情况。
12. 特征选择
    - 过滤式filter
    运用统计指标来为每个特征打分并筛选特征，再用过滤后的特征来训练模型,特征选择过程与后续学习器无关。
        + 移除低方差特征
        + 相关系数排序
        + 假设检验计算相关性
            * 卡方检验 
            卡方检验表示统计样本的实际观测值与理论推断值之间的偏离程度。sklearn-learn中有feature_selection.chi2方法支持大量特征进行卡方检验
            * F 检验
            scikit-learn 中提供了两种F检验方法 —— 适用于分类的 f_classif 和适用于回归的 f_regression ，分别对应单因素方差分析和线性相关分析
        + 互信息（信息增益）
        越大表示两个变量相关性越大。互信息能很好展现 x 和 y 之间的非线性关系。sklearn中normalized_mutual_info_score可以计算互信息
        + IV值


        + WOE值

    - 包裹式wrapper
    从初始特征集合中不断的选择特征子集，训练学习器，根据学习器的性能来对子集进行评价，直到选择出最佳的子集。效果好，但是训练开销大
    - 嵌入式embedding
    利用了模型本身的特性，将特征选择嵌入到模型的构建过程中。典型的如 Lasso 和树模型等，缺点是只有部分模型有这个功能。
13. 特征相关性


